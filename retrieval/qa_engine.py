from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import os

from retrieval.tools import retrieve_sunbeam_knowledge, refine_user_query

load_dotenv()

LLM_MODEL = "openai/gpt-oss-120b"
GROQ_BASE_URL = "https://api.groq.com/openai/v1"
GROQ_API_KEY = os.getenv("GROQ_API_KEY")


_llm = init_chat_model(
    model=LLM_MODEL,
    model_provider="openai",
    base_url=GROQ_BASE_URL,
    api_key=GROQ_API_KEY,
    temperature=0.2
)


_prompt = ChatPromptTemplate.from_messages([
    ("system", """You are an intelligent course advisor chatbot for Sunbeam Institute.

Rules:
- Use the provided context to answer questions accurately
- Never hallucinate or make up information
- Be precise and factual
- If context is insufficient, clearly state that you don't have enough information

Context:
{context}"""),
    ("human", "{question}")
])


_chain = _prompt | _llm | StrOutputParser()


def ask_question(query: str) -> str:
    """
    Answer user questions using a RAG approach.

    The function:
    1. Retrieves relevant context from the knowledge base
    2. Passes context and query to the LLM
    3. Returns the generated answer

    Args:
        query (str): User question.

    Returns:
        str: Final response generated by the LLM.
    """
    context = retrieve_sunbeam_knowledge.invoke({"query": query})
    
    if context == "NO_RELEVANT_CONTEXT":
        context = "No relevant information found in the knowledge base."
    
    response = _chain.invoke({
        "context": context,
        "question": query
    })
    
    return response